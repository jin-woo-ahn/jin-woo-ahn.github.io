<html>
<head lang="en">
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Jacob Andreas @ MIT</title>
  <link href="css/bootstrap.min.css" rel="stylesheet">
  <link href="css/style.css" rel="stylesheet">
  <link href="css/research.css" rel="stylesheet">
</head>
<body>
<div class="content">
  <h2 class="name">Jacob Andreas</h2>
  <img class="margin" src="figs/head_small.jpg" width="100%">

  <p>
    <b>I'm interested in language as a communicative and computational tool.</b>
    People learn to understand and generate novel utterances from remarkably
    little data. Having learned language, we use it acquire new concepts and to
    structure our reasoning. Current machine learning techniques fall short of
    human abilities in both their capacity to <i>learn language</i> and <i>learn
    from language</i> about the rest of the world. My research aims to (1)
    understand the computational foundations of efficient language learning,
    and (2) build general-purpose intelligent systems that can communicate
    effectively with humans and learn from human guidance.
  </p>

  <p>
    I'm the 
    <a href="https://www.x.org/wiki/XConsortium/">X Consortium</a>
    Assistant Professor at MIT in 
    <a href="http://www.eecs.mit.edu/">EECS</a> and
    <a href="http://www.eecs.mit.edu/">CSAIL</a>.
    I did my PhD work at Berkeley, where I was a member of the 
    <a href='http://nlp.cs.berkeley.edu/'>Berkeley NLP Group</a>
    and the 
    <a href="http://bair.berkeley.edu/">Berkeley AI Research Lab</a>.  
    I've also spent time with the
    <a href="http://www.cl.cam.ac.uk/research/nl/"> Cambridge NLIP Group</a>, 
    and the
    <a href="http://www.cs.columbia.edu/nlp">NLP Group</a>
    and the (erstwhile)
    Center for Computational Learning Systems
    at Columbia.
  </p>

  <p>
    <strong>Prospective students</strong>: apply through the MIT <a
    href="https://gradapply.mit.edu/eecs/apply/login/?next=/eecs/"> graduate
    admissions portal</a> in the fall. Please read my <a
    href="advising.html">advising statement</a> if you're considering applying.
    <strong>Prospective visitors</strong>: I do not currently have openings for
    visiting researchers. (I'm afraid I generally can't respond to individual
    emails about either grad admissions or visitor positions / internships.) 
  </p>

  <p>
    <a href='mailto:jda@mit.edu'>jda@mit.edu</a>,
    <a href='docs/jda_cv.pdf'>Curriculum vit&aelig;</a>,
    <a href='http://scholar.google.com/citations?user=dnZ8udEAAAAJ'>Google scholar</a>,
    <a href="http://jacobandreas.net">elsewhere</a>
  </p>

  <p>
    <a href='http://accessibility.mit.edu'>accessibility @ MIT</a>
  </p>

  <hr/>

  <h3>
    <a href="http://lingo.csail.mit.edu">Group</a> /
    <a href="http://lingo.csail.mit.edu/research.html">Research</a> /
    <a href="bio.html">Bio</a> /
    <a href="teaching/">Teaching</a>
  </h3>

  <p>
  </p>

  <hr/>

  Some current research directions:

  <div class="highlight">
    <!--<img src="figs/l3.jpg" class="margin">-->
    <h4>Learning from language</h4>
    <p>
      Much of humans' abstract knowledge comes from abstract descriptions, but
      almost all machine learning research focuses on learning from
      comparatively low-level demonstrations or interactions. How do we enable
      more natural and efficient learning from natural language supervision
      instead?
    </p>
    <p>
      <b>Papers</b>:<br/>
      <a href="https://arxiv.org/abs/2302.02801">LaMPP: Language models as probabilistic priors for perception and action</a> (preprint)<br/>
      <a href="https://arxiv.org/abs/2110.01517">Skill induction and planning with latent language</a> (ACL 2022)<br/>
      <a href="https://arxiv.org/abs/2106.11053">Leveraging language to learn program abstractions and search heuristics</a> (ICML 2021)<br/>
    </p>
  </div>

  <div class="highlight">
    <!--<img src="figs/neuralese.jpg" class="margin">-->
    <h4>Interpretation and explanation</h4>
    <p>
      How can we help humans understand the features and representational
      strategies that black-box machine learning algorithms discover? To what
      extent do these strategies reflect abstractions that we already have
      names for?
    </p>
    <p>
      <b>Papers</b>:<br/>
      <a href="https://arxiv.org/abs/2211.15661">What learning algorithm is in-context learning? Investigations with linear models</a> (ICLR 2023)<br/>
      <a href="https://arxiv.org/abs/2106.00737">Implicit representations of meaning in neural language models</a> (ACL 2021)<br/>
      <a href="https://arxiv.org/abs/2006.14032">Compositional explanations of neurons</a> (NeurIPS 2020)<br/>
    </p>
  </div>

  <div class="highlight">
    <!--<img src="figs/compositionality.jpg" class="margin">-->
    <h4>Compositionality and generalization</h4>
    <p>
      Compositionality and modularity are core features of representational
      systems in language, software and biology. How do they arise, and what
      function do they serve?  Can we use descriptions of abstract compositional
      structure in one domain (e.g. language) to learn modular representations
      in another (e.g. vision)?
    </p>
    <p>
      <b>Papers</b>:<br/>
      <a href="https://arxiv.org/abs/2211.01288">Characterizing intrinsic compositionality in transformers with tree projections</a> (ICLR 2023)<br/>
      <a href="https://arxiv.org/abs/2010.03706">Learning to recombine and resample data for compositional generalization</a> (ICLR 2021)<br/>
    </p>
  </div>

  <p>
    I'm also interested in
    <a href="https://arxiv.org/abs/1705.03919">trees</a>,
    <a href="https://www.aclweb.org/anthology/P13-1091/">graphs</a>, 
    <a href="https://arxiv.org/abs/1604.00562">games</a>,
    and
    <a href="https://papers.nips.cc/paper/5432-unsupervised-transcription-of-piano-music">sounds</a>.
  </p>

  <hr/>

  <p class='offset'>
    Collaboration graph trivia: My Erd&#337;s number is at most three 
    (J Andreas <a href="https://arxiv.org/abs/1711.02301">to</a> 
    R Kleinberg <a href="https://dl.acm.org/citation.cfm?id=1255444">to</a> 
    L Lov&aacute;sz <a href="https://www.sciencedirect.com/science/article/pii/B9780720422627500181">to</a>
    P Erd&#337;s). 
    My Kevin Bacon number (and consequently my Erd&#337;s-Bacon number) remains
    <a href="http://jacobandreas.net/misc/bacon.html">lamentably undefined</a>,
    but my Kevin Knight number (since apparently that's 
    <a href='http://www.cs.bgu.ac.il/~yoavg/uni/'>a thing</a>) 
    is one. I have never starred in a film with Kevin Knight. Noam Chomsky is
    my great-great-grand-advisor (J Andreas to D Klein to C Manning to J
    Bresnan to N Chomsky).
  </p>

  <hr/>

  <p class="credit">Photo: Gretchen Ertl / MIT CSAIL</p>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
  <script src="js/bootstrap.min.js"></script>
</div>
</body>
</html>
